import requests
import asyncio
import aiohttp
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import nest_asyncio
from pipeline.rag_chain import RAGChain
from pipeline.retriever import EmbeddingRetriever

# IMPORTANT: Permet d'utiliser asyncio.run() dans Streamlit
try:
    nest_asyncio.apply()
except:
    pass


class DistributedOrchestrator:
    """Version originale - conserv√©e pour compatibilit√©"""
    def __init__(self, node_urls: List[str], retriever: EmbeddingRetriever, model_name: str = "mistral:latest"):
        self.node_urls = node_urls
        self.rag_chain = RAGChain(retriever, model_name=model_name)

    def _query_node(self, node_url: str, query: str, k: int = 3, timeout: int = 30) -> List[Dict]:
        try:
            response = requests.post(
                f"{node_url}/search",
                json={"query": query, "k": k},
                timeout=timeout
            )
            response.raise_for_status()
            return response.json().get("results", [])
        except Exception:
            return []

    def retrieve_distributed(self, query: str, k: int = 3) -> List[Dict]:
        all_results = []

        for node_url in self.node_urls:
            node_results = self._query_node(node_url, query, k=k*2)
            all_results.extend(node_results)

        all_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        return all_results[:k]

    def generate_answer_distributed(self, query: str, k: int = 3) -> dict:
        try:
            retrieved_docs = self.retrieve_distributed(query, k=k)

            if not retrieved_docs:
                return {
                    'query': query,
                    'answer': "Les noeuds ne sont pas connect√©s.",
                    'context': []
                }

            context = self.rag_chain._format_context(retrieved_docs)
            prompt = self.rag_chain._create_prompt(query, context)
            answer = self.rag_chain._generate_with_ollama(prompt)

            return {
                'query': query,
                'answer': answer,
                'context': retrieved_docs
            }

        except Exception as e:
            return {
                'query': query,
                'answer': f"Erreur lors du traitement distribu√©: {str(e)}",
                'context': []
            }


class OptimizedDistributedOrchestrator:
    """
    Version optimis√©e avec:
    - Requ√™tes parall√®les asynchrones (CORRIG√âES pour Streamlit)
    - Cache des r√©sultats
    - Timeouts configurables
    - Gestion d'erreurs robuste
    """
    
    def __init__(self, node_urls: List[str], retriever: EmbeddingRetriever, 
                 model_name: str = "mistral:latest", use_cache: bool = True):
        self.node_urls = node_urls
        self.rag_chain = RAGChain(retriever, model_name=model_name)
        self.retriever = retriever
        self.use_cache = use_cache
        self.cache = {} if use_cache else None
        self.cache_ttl = 300  # 5 minutes
        self.node_timeout = 15  # Timeout par n≈ìud (r√©duit de 30s √† 15s)
        self.global_timeout = 25  # Timeout global (r√©duit pour plus de r√©activit√©)
        
        # ThreadPoolExecutor pour les appels synchrones (Ollama)
        self.executor = ThreadPoolExecutor(max_workers=4)
        
    # ==================== M√âTHODES ASYNCHRONES CORRIG√âES ====================
    
    async def _async_query_node(self, session: aiohttp.ClientSession, 
                                node_url: str, query: str, k: int) -> Dict[str, Any]:
        """Requ√™te asynchrone √† un n≈ìud avec gestion d'erreurs robuste"""
        try:
            async with session.post(
                f"{node_url}/search",
                json={"query": query, "k": k},
                timeout=aiohttp.ClientTimeout(total=self.node_timeout),
                headers={'Content-Type': 'application/json'}
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        'success': True,
                        'node': node_url,
                        'results': data.get("results", [])
                    }
                else:
                    print(f"‚ö†Ô∏è N≈ìud {node_url}: Status {response.status}")
                    return {'success': False, 'node': node_url, 'results': []}
        except asyncio.TimeoutError:
            print(f"‚è±Ô∏è Timeout pour le n≈ìud {node_url}")
            return {'success': False, 'node': node_url, 'results': [], 'error': 'timeout'}
        except aiohttp.ClientError as e:
            print(f"‚ùå Erreur r√©seau n≈ìud {node_url}: {str(e)}")
            return {'success': False, 'node': node_url, 'results': [], 'error': str(e)}
        except Exception as e:
            print(f"‚ùå Erreur inattendue n≈ìud {node_url}: {str(e)}")
            return {'success': False, 'node': node_url, 'results': [], 'error': str(e)}
    
    async def _async_retrieve_distributed(self, query: str, k: int = 3) -> tuple[List[Dict], int]:
        """
        R√©cup√©ration asynchrone sur tous les n≈ìuds
        Retourne: (r√©sultats, nombre de n≈ìuds r√©ussis)
        """
        # Configuration du connector pour r√©utiliser les connexions
        connector = aiohttp.TCPConnector(
            limit=len(self.node_urls),
            limit_per_host=1,
            ttl_dns_cache=300
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # Lancer toutes les requ√™tes en parall√®le
            tasks = [
                self._async_query_node(session, node_url, query, k*2)
                for node_url in self.node_urls
            ]
            
            # Attendre toutes les r√©ponses avec timeout
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True),
                    timeout=self.global_timeout
                )
            except asyncio.TimeoutError:
                print("‚è±Ô∏è Timeout global atteint")
                # R√©cup√©rer les r√©sultats partiels si possible
                results = [task.result() for task in tasks if task.done()]
            
            # Agr√©ger tous les r√©sultats
            all_results = []
            successful_nodes = 0
            
            for result in results:
                if isinstance(result, dict) and result.get('success'):
                    all_results.extend(result.get('results', []))
                    successful_nodes += 1
            
            print(f"‚úÖ {successful_nodes}/{len(self.node_urls)} n≈ìuds ont r√©pondu")
            
            # D√©dupliquer par article_number
            seen_articles = set()
            unique_results = []
            for res in all_results:
                article = res.get('article_number')
                if article and article not in seen_articles:
                    seen_articles.add(article)
                    unique_results.append(res)
                elif not article:
                    unique_results.append(res)
            
            # Trier par score de similarit√©
            unique_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
            return unique_results[:k], successful_nodes
    
    async def _async_generate_answer(self, query: str, k: int = 3) -> Dict[str, Any]:
        """G√©n√©ration de r√©ponse asynchrone compl√®te"""
        # V√©rifier le cache
        if self.use_cache:
            cache_key = f"{query}_{k}"
            if cache_key in self.cache:
                cached_data = self.cache[cache_key]
                if time.time() - cached_data['timestamp'] < self.cache_ttl:
                    print("üíæ R√©ponse trouv√©e dans le cache")
                    cached_result = cached_data['result'].copy()
                    cached_result['from_cache'] = True
                    return cached_result
        
        try:
            # R√©cup√©ration distribu√©e asynchrone
            retrieved_docs, successful_nodes = await self._async_retrieve_distributed(query, k=k)
            
            if not retrieved_docs:
                return {
                    'query': query,
                    'answer': "Les n≈ìuds ne sont pas connect√©s ou n'ont pas retourn√© de r√©sultats.",
                    'context': [],
                    'mode': 'async_failed',
                    'nodes_used': successful_nodes
                }
            
            # G√©n√©ration de la r√©ponse dans un thread s√©par√© (non-bloquant)
            loop = asyncio.get_event_loop()
            context = self.rag_chain._format_context(retrieved_docs)
            prompt = self.rag_chain._create_prompt(query, context)
            
            # Ex√©cuter Ollama dans un thread pour ne pas bloquer la boucle async
            answer = await loop.run_in_executor(
                self.executor,
                self.rag_chain._generate_with_ollama,
                prompt
            )
            
            result = {
                'query': query,
                'answer': answer,
                'context': retrieved_docs,
                'mode': 'async_distributed',
                'nodes_used': successful_nodes,
                'from_cache': False
            }
            
            # Mettre en cache
            if self.use_cache:
                self.cache[cache_key] = {
                    'result': result,
                    'timestamp': time.time()
                }
            
            return result
            
        except Exception as e:
            print(f"‚ùå Erreur async: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'query': query,
                'answer': f"Erreur lors du traitement distribu√©: {str(e)}",
                'context': [],
                'mode': 'async_error',
                'error_details': str(e)
            }
    
    def generate_answer_distributed_async(self, query: str, k: int = 3) -> Dict[str, Any]:
        """
        üöÄ M√âTHODE RECOMMAND√âE - La plus rapide (2-3x plus rapide)
        Point d'entr√©e synchrone pour la g√©n√©ration asynchrone
        Compatible avec Streamlit gr√¢ce √† nest_asyncio
        """
        try:
            # V√©rifier si une boucle existe d√©j√†
            try:
                loop = asyncio.get_running_loop()
                # Si on est d√©j√† dans une boucle async, cr√©er une t√¢che
                return asyncio.run_coroutine_threadsafe(
                    self._async_generate_answer(query, k),
                    loop
                ).result()
            except RuntimeError:
                # Pas de boucle en cours, en cr√©er une nouvelle
                return asyncio.run(self._async_generate_answer(query, k))
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution async: {str(e)}")
            # Fallback sur la version synchrone en cas d'erreur
            return self._local_fallback(query, k)
    
    # ==================== STRAT√âGIE FASTEST (CORRIG√âE) ====================
    
    async def _async_first_responder(self, query: str, k: int = 3) -> Dict[str, Any]:
        """Utilise la premi√®re r√©ponse valide (encore plus rapide)"""
        connector = aiohttp.TCPConnector(limit=len(self.node_urls))
        
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [
                self._async_query_node(session, node_url, query, k)
                for node_url in self.node_urls
            ]
            
            # Attendre la premi√®re r√©ponse r√©ussie
            for coro in asyncio.as_completed(tasks):
                try:
                    result = await coro
                    if result.get('success') and result.get('results'):
                        print(f"‚ö° Premier n≈ìud: {result['node']}")
                        
                        # Utiliser imm√©diatement ces r√©sultats
                        retrieved_docs = result['results'][:k]
                        
                        # G√©n√©rer la r√©ponse dans un thread
                        loop = asyncio.get_event_loop()
                        context = self.rag_chain._format_context(retrieved_docs)
                        prompt = self.rag_chain._create_prompt(query, context)
                        answer = await loop.run_in_executor(
                            self.executor,
                            self.rag_chain._generate_with_ollama,
                            prompt
                        )
                        
                        return {
                            'query': query,
                            'answer': answer,
                            'context': retrieved_docs,
                            'fastest_node': result['node'],
                            'mode': 'first_responder'
                        }
                except Exception:
                    continue
            
            # Aucun n≈ìud n'a r√©pondu - fallback local
            return self._local_fallback(query, k)
    
    def generate_answer_fastest(self, query: str, k: int = 3) -> Dict[str, Any]:
        """
        ‚ö° STRAT√âGIE LA PLUS RAPIDE (3-5x plus rapide)
        Utilise la premi√®re r√©ponse valide
        """
        try:
            try:
                loop = asyncio.get_running_loop()
                return asyncio.run_coroutine_threadsafe(
                    self._async_first_responder(query, k),
                    loop
                ).result()
            except RuntimeError:
                return asyncio.run(self._async_first_responder(query, k))
        except Exception as e:
            print(f"‚ùå Erreur fastest: {str(e)}")
            return self._local_fallback(query, k)
    
    # ==================== STRAT√âGIE THREADPOOL (PLUS S√õRE) ====================
    
    def _threaded_query_node(self, node_url: str, query: str, k: int) -> Dict[str, Any]:
        """Requ√™te synchrone pour ThreadPool"""
        try:
            response = requests.post(
                f"{node_url}/search",
                json={"query": query, "k": k},
                timeout=self.node_timeout,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            return {
                'success': True,
                'node': node_url,
                'results': response.json().get("results", [])
            }
        except requests.Timeout:
            print(f"‚è±Ô∏è Timeout pour le n≈ìud {node_url}")
            return {'success': False, 'node': node_url, 'results': [], 'error': 'timeout'}
        except Exception as e:
            print(f"‚ùå Erreur n≈ìud {node_url}: {str(e)}")
            return {'success': False, 'node': node_url, 'results': [], 'error': str(e)}
    
    def generate_answer_threaded(self, query: str, k: int = 3) -> Dict[str, Any]:
        """
        üîÑ ALTERNATIVE RECOMMAND√âE (1.5-2x plus rapide)
        Utilise ThreadPoolExecutor - PLUS STABLE avec Streamlit
        """
        try:
            # Requ√™tes parall√®les avec ThreadPool
            with ThreadPoolExecutor(max_workers=len(self.node_urls)) as executor:
                futures = {
                    executor.submit(self._threaded_query_node, node_url, query, k*2): node_url
                    for node_url in self.node_urls
                }
                
                all_results = []
                successful_nodes = 0
                
                for future in as_completed(futures, timeout=self.global_timeout):
                    try:
                        result = future.result(timeout=1)  # Timeout additionnel
                        if result.get('success'):
                            all_results.extend(result.get('results', []))
                            successful_nodes += 1
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erreur future: {str(e)}")
                        continue
                
                print(f"‚úÖ {successful_nodes}/{len(self.node_urls)} n≈ìuds ont r√©pondu")
                
                if not all_results:
                    return {
                        'query': query,
                        'answer': "Les n≈ìuds ne sont pas connect√©s.",
                        'context': [],
                        'mode': 'threaded_failed',
                        'nodes_used': successful_nodes
                    }
                
                # D√©dupliquer et trier
                seen_articles = set()
                unique_results = []
                for res in all_results:
                    article = res.get('article_number')
                    if article and article not in seen_articles:
                        seen_articles.add(article)
                        unique_results.append(res)
                    elif not article:
                        unique_results.append(res)
                
                unique_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
                retrieved_docs = unique_results[:k]
                
                # G√©n√©rer la r√©ponse
                context = self.rag_chain._format_context(retrieved_docs)
                prompt = self.rag_chain._create_prompt(query, context)
                answer = self.rag_chain._generate_with_ollama(prompt)
                
                return {
                    'query': query,
                    'answer': answer,
                    'context': retrieved_docs,
                    'nodes_used': successful_nodes,
                    'mode': 'threaded_distributed'
                }
                
        except Exception as e:
            print(f"‚ùå Erreur threaded: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'query': query,
                'answer': f"Erreur lors du traitement distribu√©: {str(e)}",
                'context': [],
                'mode': 'threaded_error'
            }
    
    # ==================== M√âTHODE PAR D√âFAUT ====================
    
    def generate_answer_distributed(self, query: str, k: int = 3) -> Dict[str, Any]:
        """
        M√©thode par d√©faut - Utilise ThreadPool (PLUS STABLE pour Streamlit)
        Change en async si vous √™tes s√ªr de la compatibilit√©
        """
        return self.generate_answer_threaded(query, k)
    
    # ==================== UTILITAIRES ====================
    
    def _local_fallback(self, query: str, k: int) -> Dict[str, Any]:
        """Fallback sur le retriever local si tous les n≈ìuds √©chouent"""
        try:
            print("üîÑ Fallback sur recherche locale")
            retrieved_docs = self.retriever.retrieve(query, k=k)
            
            context = self.rag_chain._format_context(retrieved_docs)
            prompt = self.rag_chain._create_prompt(query, context)
            answer = self.rag_chain._generate_with_ollama(prompt)
            
            return {
                'query': query,
                'answer': answer,
                'context': retrieved_docs,
                'mode': 'local_fallback'
            }
        except Exception as e:
            return {
                'query': query,
                'answer': f"Erreur: {str(e)}",
                'context': [],
                'mode': 'error'
            }
    
    def clear_cache(self):
        """Vider le cache"""
        if self.use_cache and self.cache:
            self.cache.clear()
            print("üóëÔ∏è Cache vid√©")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Statistiques du cache"""
        if not self.use_cache or not self.cache:
            return {'enabled': False}
        
        valid_entries = sum(
            1 for data in self.cache.values()
            if time.time() - data['timestamp'] < self.cache_ttl
        )
        
        return {
            'enabled': True,
            'total_entries': len(self.cache),
            'valid_entries': valid_entries,
            'expired_entries': len(self.cache) - valid_entries
        }
    
    def health_check(self) -> Dict[str, Any]:
        """V√©rifier la sant√© des n≈ìuds (synchrone, rapide)"""
        status = {}
        for node_url in self.node_urls:
            try:
                response = requests.get(
                    f"{node_url}/health", 
                    timeout=2,
                    headers={'Content-Type': 'application/json'}
                )
                status[node_url] = {
                    'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                    'code': response.status_code
                }
            except requests.Timeout:
                status[node_url] = {
                    'status': 'timeout',
                    'error': 'Timeout after 2s'
                }
            except Exception as e:
                status[node_url] = {
                    'status': 'unreachable',
                    'error': str(e)
                }
        return status
    
    def __del__(self):
        """Cleanup du ThreadPoolExecutor"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)